<!DOCTYPE HTML>
<!--
	Stellar by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Logistic Regression | Sarah Nagy</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<h1>Logistic Regression for Kaggle's Titanic Survivor Prediction Competition</h1>
						

				<!-- Main -->
					<div id="main">

						<!-- Content -->
							<section id="content" class="main">
								<!-- <span class="image main"><img src="images/pic04.jpg" alt="" /></span> -->
								<h2>Background and Goals</h2>
								<p>To practice data cleaning and logistic regression for predictive modeling, I participated in Kaggle's popular Titanic Survivor Prediction competition. The premise is to predict whether passengers survived based on a data set containing passenger's names, ages, genders, number of family members aboard the ship, ticket class, ticket I.D., fare paid, and location of embarkment. Kaggle provides a large training set including survival information and a smaller test set to submit back to Kaggle with your predictions. This project required me to clean the data by finding appropriate replacements for missing data, and transforming categorical variables into binary operators. I also practiced randomizing the training set to get a smaller trainging and set set from my given data, applying logistic regression, and analyzing the results. I used Python for the data cleaning and R for the data analysis.</p>
								<h2>Getting Started</h2>
								<p>First I did some exploratory analysis on this data and found that there were missing values in the age and embarkment location categories. I also found that sex and embarkment location were non-numerical categorical variables. First I  to find missing values and information about the categories with missing values. I found that there were missing values in age, </p>								
								<p>The following is the code I used for this step:</p>
								<pre><code>surv = open('test.csv','r')
new = open('test_ready.csv', 'w')

def get_ready(surv, new):
    new.writelines(['Pclass,','is_female,','is_male,','age,','SibSp,','Parch,','Fare,','emb_is_Q,','emb_is_S,','emb_is_C,','Survived\n'])
    surv.readline()
    for line in surv.readlines():
        new_values = []
        values = line.split(",")
        new_values.append(str(values[2])+",")
        if values[5] == 'female':
            new_values.append("1,")
            new_values.append("0,")
        elif values[5] == 'male':
            new_values.append("0,")
            new_values.append("1,")
        else:
            new_values.append(",")
        new_values.append(str(values[6])+",")
        new_values.append(str(values[7])+",")
        new_values.append(str(values[8])+",")
        new_values.append(str(values[10])+",")
        if values[12] == 'Q\n':
            new_values.append("1,")
            new_values.append("0,")
            new_values.append("0,")
        elif values[12] == 'S\n':
            new_values.append("0,")
            new_values.append("1,")
            new_values.append("0,")
        elif values[12] == 'C\n':
            new_values.append("0,")
            new_values.append("0,")
            new_values.append("1,")
        else:
            new_values.append(",")
            new_values.append(",")
            new_values.append(",")
        new_values.append(str(values[1])+"\n")
        new.writelines(new_values)
get_ready(surv, new)

surv.close()
new.close()
</pre></code>
								<p>With the data is split up, I tested out a simple linear function with no regularization. I used the great fmincg function designed by Carl Rasmussen to minimize the cost function of the proposed model using and trained the model using gradient descent. Then I evaluated the model with the Root Mean Squared Error and normalized RMSE.</p>
								<pre><code>%test a linear function with no regularization

init_theta = ones(n+1, 1);
%J = 0;
%X_grad = zeros(size(X));
%Theta_grad = zeros(size(init_theta));

%lambda
%lambda = 40
lambda= 0
[J, grad] = linearRegCostFunction(X_train, y_train, init_theta, lambda);
% Set Options
options = optimset('GradObj', 'on', 'MaxIter', 400);

% Optimize using Rasmussen's fminunc
[theta] = trainLinearReg([ones(train_size, 1) X_train], y_train, lambda);

%apply this proposed theta to the cross-validation group
predictions = X_cv * theta(2:end, :);

%generate MeanSquared Error and Normalized Mean Squared Error to evaluate the prediction ability of this model
MSE_cv = (sum((predictions - y_cv).^2)/cv_size)
RMSE = sqrt(MSE_cv)
n_RMSE_cv = RMSE/(max(y_cv)-min(y_cv))</pre></code>
								<p>Since I fit the model to the training group, I tested it out on the cross validation group. It resulted in a Normalized RMSE of ~.17. Next, I investigated adding a regularization term as well as higher degree order polynomials. The regularization term helps address overfitting; adding a constant in front of each term means that the cost will increase by weighting the feature more heavily. This serves to prevent overfitting on the training group. Adding polynomial terms addressing the opposite: underfitting. Polynomial terms allow the model to more precisely fit the data. In the next steps, I test out various values for lambda (the regularization term) and t (the order of the polynomials).</p>
								<pre><code>%vary the highest polynomial degree with this variable:
p = 30;

%insert proposed lambda values here:
lam = [0, .0001, .0005, .001, .005 .01, .05, .1, .5, 1, 5];



FX_train = [];
FX_test = [];
for t = 1:30
  counter = 1;
  n_degree_train = X_train.^t;
  n_degree_test = X_test.^t;
  FX_train = [FX_train n_degree_train];
  FX_test = [FX_test n_degree_test];
  [m , n] = size(FX_train);
  init_theta = ones(n, 1);
  for lambda = lam
    [J, grad] = linearRegCostFunction(FX_train, y_train, init_theta, lambda);
    [theta] = trainLinearReg( FX_train, y_train, lambda);
    pred_train = FX_train * theta;
    MSE_train = (sum((pred_train - y_train).^2)/train_size);
    pred_test = FX_test* theta;
    MSE_test = (sum((pred_test - y_test).^2)/test_size);
    MSE_test_vector(t,counter) = MSE_test ;
    MSE_train_vector(t, counter) = MSE_train;
    lambda_matrix(t, counter) = lambda;
    degree_matrix(t, counter) = t;
    counter += 1;
    endfor

endfor
MSE_train_vector;
MSE_test_vector;
%%%Make 3 matrices all the same size with the lambda and degree of each cost J

surf(lambda_matrix,degree_matrix, MSE_train_vector)
title("plot of training set");
xlabel("lambda");
ylabel("degree of polynomial");
zlabel("Mean squared error");
surf(lambda_matrix,degree_matrix, MSE_test_vector)
title("plot of test set");
xlabel("lambda");
ylabel("degree of polynomial");
zlabel("Mean squared error");
</pre></code>
								<p>I ran this a handful of times, following potential minimum RMSE values. I found that a 4th degree polynomial and regularization value of .1 resulted in the best fit for this data. I then tested this proposed model on my third section of the data, the test set. The reason for doing this is to make sure that I haven't overfit these new parameters to the cross-validation set, meaning that the degree of the polynomial and lambda value work best for the training set, but wouldn't actually generalize well. If this is the case, the RMSE would be significantly lower for the cross-validation set than the test set. If they RMSEs for the two groups are similar, then we hava a good indication that the model will preform just as well on new data sets as it does on the test set (given, of course, that the data I am using is a representative sample of the population). </p>
								<pre><code>FX_train = [];
FX_test = [];
FX_cv = [];
for t = 1:4
  n_degree_train = X_train.^t;
  n_degree_test = X_test.^t;
  n_degree_cv = X_cv.^t;
  FX_train = [FX_train n_degree_train];
  FX_test = [FX_test n_degree_test];
  FX_cv = [FX_cv n_degree_cv];
  [m , n] = size(FX_train);
  init_theta = ones(n, 1);


[J, grad] = linearRegCostFunction(FX_train, y_train, init_theta, .1);
[theta] = trainLinearReg( FX_train, y_train, .1);
pred_train = FX_train * theta;
MSE_train = (sum((pred_train - y_train).^2)/train_size)
pred_test = FX_test* theta;
MSE_test = (sum((pred_test - y_test).^2)/test_size)
pred_cv = FX_cv* theta;
MSE_cv = (sum((pred_cv - y_cv).^2)/cv_size)

%%They all are pretty similar, so this means we have found a good fit that will
%%likely work just as well on new data
</pre></code>
									<p>The MSREs calculated for each set was pretty similar, around ADD VALUE!!!!!!!!!!, so this is a good indication of how well this model will preform on new data.</p>
									<h2>Conclusion and Next Steps</h2>
									<p>The MSRE and NMSRE provide a way to compare different models to see which is a better fit, because they are a way of measuring the difference between the predicted and actual observed values. NMSRE values range from 0 to 1, where a 0 would mean that there was no difference between the predictions and observed values. The NMSRE value that I found is not particularly close to 0. In order to lower this value, a next possible step could be to try ignoring some of the features to see if some of them are actually hindering our fit. If this doesn't work, we could try to gather more data in order ot have a larger training set. Whether or not we would take these steps depends on how accurate this model needs to be. A NMRSE of INSERT VALUE HERE!!!!!!!!! may be satisfactory in some situations.</p>
							</section>

					</div>

				<!-- Footer -->
					<footer id="footer">
						<section>
							<h2>Aliquam sed mauris</h2>
							<p>Sed lorem ipsum dolor sit amet et nullam consequat feugiat consequat magna adipiscing tempus etiam dolore veroeros. eget dapibus mauris. Cras aliquet, nisl ut viverra sollicitudin, ligula erat egestas velit, vitae tincidunt odio.</p>
							<ul class="actions">
								<li><a href="#" class="button">Learn More</a></li>
							</ul>
						</section>
						<section>
							<h2>Etiam feugiat</h2>
							<dl class="alt">
								<dt>Location</dt>
								<dd>Bozeman, MT 59715 &bull; USA</dd>
								<dt>Email</dt>
								<dd><a href="#">snsarahmarienagy@gmail.com</a></dd>
							</dl>
							<ul class="icons">
								<li><a href="https://github.com/SarahMari" class="icon brands fa-github alt"><span class="label">GitHub</span></a></li>
								<li><a href="#" class="icon brands fa-dribbble alt"><span class="label">Dribbble</span></a></li>
							</ul>
						</section>
						<p class="copyright">&copy; Untitled. Design: <a href="https://html5up.net">HTML5 UP</a>.</p>
					</footer>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
