<!DOCTYPE HTML>
<!--
	Stellar by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Logistic Regression | Sarah Nagy</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<h1>Logistic Regression for Kaggle's Titanic Survivor Prediction Competition</h1>
						
					</header>
				<!-- Main -->
					<div id="main">

						<!-- Content -->
							<section id="content" class="main">
								<!-- <span class="image main"><img src="images/pic04.jpg" alt="" /></span> -->
								<h2>Background and Goals</h2>
								<p>To practice data cleaning and logistic regression for predictive modeling, I participated in Kaggle's popular Titanic Survivor Prediction competition. The premise is to predict whether passengers survived the Titanic incident after training a model with a data set containing passenger's names, ages, genders, number of family members aboard the ship, ticket class, ticket I.D., fare paid, and location of embarkment. Kaggle provides a large training set including survival information and a smaller test set to submit back to Kaggle with your predictions. This project required me to clean the data by finding appropriate replacements for missing data, and transforming categorical variables into binary operators. I also practiced randomizing the training set to get a smaller training set from my given data, using logistic regression, and analyzing the results. I used Python for the data cleaning and R for the data analysis.</p>
								<h2>Getting Started</h2>
								<p>First I did some exploratory analysis on this data and found that there were missing values in the age and embarkment location categories. I also found that sex and embarkment location were non-numerical categorical variables, while the rest were quantitative. I wrote the following Python code to read the given training data Excel sheet and return a new Excel file with new categories, "is_female", "is_male", "emb_is_Q" for an embarkment location of Queenstown, "emb_is_S" for Southampton, and "emb_is_C" for Cherbourg. These new variables allow for the use of binary operators, which will be very useful for the logistic regression later on.   </p>								
								<p>The following is the Python code I wrote for this step:</p>
								<pre><code>surv = open('test.csv','r')
new = open('test_ready.csv', 'w')

def get_ready(surv, new):
    new.writelines(['Pclass,','is_female,','is_male,','age,','SibSp,','Parch,','Fare,','emb_is_Q,','emb_is_S,','emb_is_C,','Survived\n'])
    surv.readline()
    for line in surv.readlines():
        new_values = []
        values = line.split(",")
        new_values.append(str(values[2])+",")
        if values[5] == 'female':
            new_values.append("1,")
            new_values.append("0,")
        elif values[5] == 'male':
            new_values.append("0,")
            new_values.append("1,")
        else:
            new_values.append(",")
        new_values.append(str(values[6])+",")
        new_values.append(str(values[7])+",")
        new_values.append(str(values[8])+",")
        new_values.append(str(values[10])+",")
        if values[12] == 'Q\n':
            new_values.append("1,")
            new_values.append("0,")
            new_values.append("0,")
        elif values[12] == 'S\n':
            new_values.append("0,")
            new_values.append("1,")
            new_values.append("0,")
        elif values[12] == 'C\n':
            new_values.append("0,")
            new_values.append("0,")
            new_values.append("1,")
        else:
            new_values.append(",")
            new_values.append(",")
            new_values.append(",")
        new_values.append(str(values[1])+"\n")
        new.writelines(new_values)
get_ready(surv, new)

surv.close()
new.close()
</pre></code>
								<h2>Model Building</h2>
								<p>Now that I had all of the data in a useable form, I looked at missing data and associated statistics. I moved over to RStudio to take advantage of some of its great built in functions. Using the summary function, I found that both age and embarkment location had missing data. I decided to replace the missing age data with the median age- 28 years old. I chose the median because it is more resistant to outliers than mean. I decided to replace missing embarkment location values with the most commonly occuring embarkment location- Southampton. I used the following code to take care of the missing data problem.</p>
								<pre><code>train$age[is.na(train$age)]<- 28
train$emb_is_Q[is.na(train$emb_is_Q)] <- 0
train$emb_is_C[is.na(train$emb_is_C)] <- 0
train$emb_is_S[is.na(train$emb_is_S)] <- 1</pre></code>
								<p>I ran R's summary function again for another check and found there was no missing data and that each variable was in a quantitative form. Next I moved over to GNU Octave for the proper anlysis. First, my function loads and stores the data. Then it spilts off the survival data column into a y-vector. Then I apply feature scaling and mean normalization, and randomly group the data into a training set(60%), test set(20%), and cross-validation set(20%). Feature scaling and mean normalization are helpful for training a better model since features with larger scales won't be arbitrarily weighted more heavily. Random assigment to train, test, and cross validation groups allows me to train the model on the training set, optimize some parameters with the test set, and then test those parameters with the cross-validation set</p>
								<pre><code>
								function [theta accuracy_test] = titanic(file_name)

%The goal of this program is to take the titanic survival data that I pre-processed and find a function that can accruately predict survival on new data.
%%% I will do this by training a linear regression algorithm on a training set, using a cross-validation set to optimize the degree of the polynomials and lambda values


%Read and load the data
data = csvread(file_name);
[a ,b] = size(data);
%split into X and Y
X = data(:,1:(b-1));
y = data(:,b);
%initialize helpful values
[m, n] = size(X);

%feature scaling brings all of the features onto similar scales, preventing features on a larger scale from being unnecessarily heavily weighted

for i = 1:n
  X(:,i) = X(:,i)./(max(X(:,i)));
  endfor

%mean normalization adjusts each feature value so that the mean value for each feature is 0

for i = 1:n
  X(:,i) = X(:,i)-mean(X(:,i));
endfor

% Add intercept term to X and X_test
X = [ones(m, 1) X];

X(1:5,:);

%randomize training set

%the size of the train, cv, and test will be 60%, 20%, and 20% of the data set examples, respectively
train_size = round(m*.6);
cv_size = round(m*.2);
test_size = m - (cv_size +train_size);

%I want to make this random because I don't know if this data is ordered in some way
%so I don't want to just chose the fist 60% to be the training set
%so I chose a random permutation of train_size numbers within the range of 0-m
rand_sample = randperm(m, train_size);

%set X_train and y_train to be these random examples
X_train = X(rand_sample,:);
y_train = y(rand_sample, :);

%need to define a variable to be the leftover set of the remaining 40% of the data

%name a new variable that contains X
X_leftover = X;

%remove the training examples
X_leftover([rand_sample],:) = [];

%do the same for y
y_leftover = y;
y_leftover([rand_sample],:) = [];

%chose a random permutation of cv_size number of numbers within the range of
%m- train_size, aka the leftover size
rs_lo = randperm((m-train_size), cv_size);

%define X_cv and y_cv as these random examples, chosen from the leftover set
X_cv = X_leftover(rs_lo,:);
y_cv = y_leftover(rs_lo, :);

%set X_test as equal to the leftover set, then remove the examples chosen for X_cv
X_test = X_leftover;
X_test([rs_lo],:) = [];
%do the same for y_test
y_test = y_leftover;
y_test([rs_lo], :) = [];

%test a linear function with no regularization

init_theta = ones(n+1, 1);
%J = 0;
%X_grad = zeros(size(X));
%Theta_grad = zeros(size(init_theta));

%lambda
%lambda = 40
lambda= 0
[J, grad] = linearRegCostFunction(X_train, y_train, init_theta, lambda);
% Set Options
options = optimset('GradObj', 'on', 'MaxIter', 400);

% Optimize using Rasmussen's fminunc
[theta] = trainLinearReg([ones(train_size, 1) X_train], y_train, lambda);

%apply this proposed theta to the cross-validation group
predictions = X_cv * theta(2:end, :);
%predictions = X * theta(2:end, :);
predictions(1:15)
r_p = round(predictions);
r_p(1:15)
l_p = predictions;
for i = 1:cv_size
    if l_p(i) >= .5;
      l_p(i) = 1;
    else
      l_p(i) = 0;
    endif
    endfor

l_p(1:15)
sum(l_p != r_p)
%accuracy = mean(y_cv == predictions)

%immediately we get an %80 accuracy. Investigate how changing lambda and degree will imrpove it

%vary the highest polynomial degree with this variable:
p = 30;

%insert proposed lambda values here:
lam = [0, .0001, .0003, .0005, .0008, .001, .003,.005, .008 .01, ];



FX_train = [];
FX_test = [];
for t = 1:30
  counter = 1;
  n_degree_train = X_train.^t;
  n_degree_test = X_test.^t;
  FX_train = [FX_train n_degree_train];
  FX_test = [FX_test n_degree_test];
  [m , n] = size(FX_train);
  init_theta = ones(n, 1);
  for lambda = lam
    [J, grad] = linearRegCostFunction(FX_train, y_train, init_theta, lambda);
    [theta] = trainLinearReg( FX_train, y_train, lambda);
    pred_train = FX_train * theta;
    %I'm going to try minimizing the cost this time instead of the accuracy even though they should be v correlated
    [J_train, grad] = linearRegCostFunction(FX_train, y_train, theta, lambda);
    J_train_vector(t,counter) = J_train ;
    [J_test, grad] = linearRegCostFunction(FX_test, y_test, theta, lambda);
    J_test_vector(t, counter) = J_test;
    lambda_matrix(t, counter) = lambda;
    degree_matrix(t, counter) = t;
    counter += 1;
    endfor

endfor
%MSE_train_vector;
%MSE_test_vector;
%%%Make 3 matrices all the same size with the lambda and degree of each cost J

surf(lambda_matrix,degree_matrix, J_train_vector)
title("plot of training set");
xlabel("lambda");
ylabel("degree of polynomial");
zlabel("Cost(J)");
surf(lambda_matrix,degree_matrix, J_test_vector)
title("plot of test set");
xlabel("lambda");
ylabel("degree of polynomial");
zlabel("Cost(J)");

%these plots show that there is generally a minimum around a 5th order polynomial and lmbda is
%about equal to .0001

%finally I will run that NMSE equation on each of the sets, including the CV, to make
%that the choices made using the test set provide good predictors for the cross validation set



FX_train = [];
FX_test = [];
FX_cv = [];
for t = 1:5
  n_degree_train = X_train.^t;
  n_degree_test = X_test.^t;
  n_degree_cv = X_cv.^t;
  FX_train = [FX_train n_degree_train];
  FX_test = [FX_test n_degree_test];
  FX_cv = [FX_cv n_degree_cv];
  [m , n] = size(FX_train);
  init_theta = ones(n, 1);
  endfor

[J, grad] = linearRegCostFunction(FX_train, y_train, init_theta, .1);
[theta] = trainLinearReg( FX_train, y_train, .1);
pred_train = round(FX_train * theta);
accuracy_train = mean(pred_train == y_train)
pred_test = round(FX_test* theta);
accuracy_test = mean(pred_test == y_test)
pred_cv = round(FX_cv* theta);
accuracy_cv = mean(pred_cv == y_cv)
end

</pre></code>
									<p>This model provided a 78% prediction accuracy for the test set. </p>
									<h2>Conclusion and Next Steps</h2>
									<p>In order to increase the prediction accuracy, I would like to analyize the model for under or over fitting. The results of this analysis would give information about which methods may help increase accuracy. These steps may include: testing out different strategies with the lambda and polynomial order to get a better fit, removing or adding some features, or simply getting more data. I could run some analysis on the features to see if they overlap significantly in interaction and explanability. If so, I would like to experiment with removing some of those with great interaction. Gathering more data is the least attractive action as it generally requires much more time and effort on behalf of the data science team, so it is much more favorable to explore manipulations within the existing model and data first.</p>
							</section>

					</div>

				<!-- Footer -->
					<footer id="footer">
						<section>
							<h2>About Me</h2>
							<p>I am a data scientist from Montana with a background in physics and statistics. I am very interested in machine learning and big data analysis. Thanks for taking the time to look at my work. Please reach out if you have any questions for me. </p>
							<ul class="actions">
							</ul>
						</section>
						<section>
							<h2>Information</h2>
							<dl class="alt">
								<dt>Address</dt>
								<dd>Bozeman, MT &bull; USA</dd>
								<dt>Email</dt>
								<dd><a href="#">s.nagy.4343@gmail.com</a></dd>
							</dl>
							<ul class="icons">
								<li><a href="https://github.com/SarahMari" class="icon brands fa-github alt"><span class="label">GitHub</span></a></li>
								<li><a href= "http://linkedin.com/in/sarah-nagy-51412152" class = "icon brands fa-linkedin alt"><span class ="label"><Linkedin</span></a></li>
							</ul>
						</section>
						
					</footer>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
