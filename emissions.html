<!DOCTYPE HTML>
<!--
	Stellar by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Exploratory Data Analysis on Emissions Data| Sarah Nagy</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<span class="logo"><img src="images/logo.svg" alt="" /></span>
						<p>   </p>
						<h1>Exploratory Data Analysis on Emissions Data</h1>
						<p> For this project, I used data Influence Map's Carbon Majors Database. I wanted to showcase my data visualization skills and practice data exploration.</p>
					</header>

				<!-- Main -->
					<div id="main">

						<!-- Content -->
							<section id="content" class="main">
								<!-- <span class="image main"><img src="images/pic04.jpg" alt="" /></span> -->
								<h2>Background and Goals</h2>
								<p>First I took a look at the general information surrounding the data.
                <p>I used the following code for this step:</p>
								<pre><code>pd.options.display.max_columns = None
df = pd.read_csv("emissions_high_granularity.csv") 
print(df.head()) 
</pre></code>
								<img src="data_head.png" alt="Data Head">
								<p>This function returns a new CSV file "insurance_ready.csv" which can be used for further analysis. There is no missing data in this dataset, so no further preprocessing is required.</p>
								<h2>Model Building in Octave</h2>
								<p>First, my function loads and stores the data. Then it splits off the amount of charges billed to insurance column into a y-vector. Then I apply feature scaling and mean normalization, and randomly group the data into a training set(60%), test set(20%), and cross-validation set(20%). Feature scaling and mean normalization help gradient descent converge more quickly, as well as help train a better model since features on larger scales won't be arbitrarily weighted more heavily. Random assignment to train, test, and cross validation groups allows me to train the model on the training set, optimize some parameters with the test set, and then test those parameters with the cross-validation set.</p>
								<pre><code>function prediction = insurance(file_name)

%The goal of this program is to take the insurance data that I pre-processed and find a function that can accurately predict insurance charges on new data.
%%% I will do this by training a linear regression algorithm on a training set, using a cross-validation set to optimize the degree of the polynomials and lambda values


%Read and load the data
data = csvread(file_name);
[a ,b] = size(data);
%split into X and Y
X = data(:,1:(b-1));
y = data(:,b);
%initialize helpful values
[m, n] = size(X);

%feature scaling brings all of the features onto similar scales, preventing features on a larger scale from being unnecessarily heavily weighted

for i = 1:n
  X(:,i) = X(:,i)./(max(X(:,i)));
  endfor

%mean normalization adjusts each feature value so that the mean value for each feature is 0

for i = 1:n
  X(:,i) = X(:,i)-mean(X(:,i));
endfor

% Add intercept term to X and X_test
X = [ones(m, 1) X];

%randomize training set

%the size of the train, cv, and test will be 60%, 20%, and 20% of the data set examples, respectively
train_size = round(m*.6);
cv_size = round(m*.2);
test_size = m - (cv_size +train_size);

%I want to make this random because I don't know if this data is ordered in some way
%so I don't want to just chose the fist 60% to be the training set
%so I chose a random permutation of train_size numbers within the range of 0-m
rand_sample = randperm(m, train_size);

%set X_train and y_train to be these random examples
X_train = X(rand_sample,:);
y_train = y(rand_sample, :);

%need to define a variable to be the leftover set of the remaining 40% of the data

%name a new variable that contains X
X_leftover = X;

%remove the training examples
X_leftover([rand_sample],:) = [];

%do the same for y
y_leftover = y;
y_leftover([rand_sample],:) = [];

%chose a random permutation of cv_size number of numbers within the range of
%m- train_size, aka the leftover size
rs_lo = randperm((m-train_size), cv_size);

%define X_cv and y_cv as these random examples, chosen from the leftover set
X_cv = X_leftover(rs_lo,:);
y_cv = y_leftover(rs_lo, :);

%set X_test as equal to the leftover set, then remove the examples chosen for X_cv
X_test = X_leftover;
X_test([rs_lo],:) = [];
%do the same for y_test
y_test = y_leftover;
y_test([rs_lo], :) = [];</pre></code>
								<p>With the data is split up, I tested out a simple linear function with no regularization. I used the "fmincg" function designed by Carl Rasmussen to minimize the cost function of the proposed model using and trained the model using gradient descent. Then I evaluated the model with the Root Mean Squared Error and normalized RMSE.</p>
								<pre><code>%test a linear function with no regularization

init_theta = ones(n+1, 1);
%J = 0;
%X_grad = zeros(size(X));
%Theta_grad = zeros(size(init_theta));

%lambda
%lambda = 40
lambda= 0
[J, grad] = linearRegCostFunction(X_train, y_train, init_theta, lambda);
% Set Options
options = optimset('GradObj', 'on', 'MaxIter', 400);

% Optimize using Rasmussen's fminunc
[theta] = trainLinearReg([ones(train_size, 1) X_train], y_train, lambda);

%apply this proposed theta to the cross-validation group
predictions = X_cv * theta(2:end, :);

%generate Mean Squared Error and Normalized Mean Squared Error to evaluate the prediction ability of this model
MSE_cv = (sum((predictions - y_cv).^2)/cv_size)
RMSE = sqrt(MSE_cv)
n_RMSE_cv = RMSE/(max(y_cv)-min(y_cv))</pre></code>
								<p>Since I fit the model to the training group, I tested it out on the cross validation group. It resulted in a Normalized RMSE of ~.17. Next, I investigated adding a regularization term as well as higher degree order polynomials. The regularization term helps address overfitting; adding a constant in front of each term means that the cost will increase by weighting the feature more heavily. This serves to prevent overfitting on the training group. Adding polynomial terms addressing the opposite: underfitting. Polynomial terms allow the model to more precisely fit the data. In the next steps, I test out various values for lambda (the regularization term) and t (the order of the polynomials).</p>
								<pre><code>%vary the highest polynomial degree with this variable:
p = 30;

%insert proposed lambda values here:
lam = [0, .0001, .0005, .001, .005 .01, .05, .1, .5, 1, 5];

FX_train = [];
FX_test = [];
for t = 1:30
  counter = 1;
  n_degree_train = X_train.^t;
  n_degree_test = X_test.^t;
  FX_train = [FX_train n_degree_train];
  FX_test = [FX_test n_degree_test];
  [m , n] = size(FX_train);
  init_theta = ones(n, 1);
  for lambda = lam
    [J, grad] = linearRegCostFunction(FX_train, y_train, init_theta, lambda);
    [theta] = trainLinearReg( FX_train, y_train, lambda);
    pred_train = FX_train * theta;
    MSE_train = (sum((pred_train - y_train).^2)/train_size);
    pred_test = FX_test* theta;
    MSE_test = (sum((pred_test - y_test).^2)/test_size);
    MSE_test_vector(t,counter) = MSE_test ;
    MSE_train_vector(t, counter) = MSE_train;
    lambda_matrix(t, counter) = lambda;
    degree_matrix(t, counter) = t;
    counter += 1;
    endfor

endfor
MSE_train_vector;
MSE_test_vector;
%%%Make 3 matrices all the same size with the lambda and degree of each cost J

surf(lambda_matrix,degree_matrix, MSE_train_vector)
title("plot of training set");
xlabel("lambda");
ylabel("degree of polynomial");
zlabel("Mean squared error");
surf(lambda_matrix,degree_matrix, MSE_test_vector)
title("plot of test set");
xlabel("lambda");
ylabel("degree of polynomial");
zlabel("Mean squared error");
</pre></code>
								<p>I ran this a handful of times, following potential minimum RMSE values. I found that a 4th degree polynomial and regularization value of .1 resulted in the best fit for this data. I then tested this proposed model on my third section of the data, the test set. The reason for doing this is to make sure that I haven't overfit these new parameters to the cross-validation set, meaning that the degree of the polynomial and lambda value work best for the training set, but wouldn't actually generalize well. If this is the case, the RMSE would be significantly lower for the cross-validation set than the test set. If they RMSEs for the two groups are similar, this is a good indication that the model will preform just as well on new data sets as it does on the test set (given, of course, that the data is a representative sample of the population). </p>
								<pre><code>FX_train = [];
FX_test = [];
FX_cv = [];
for t = 1:4
  n_degree_train = X_train.^t;
  n_degree_test = X_test.^t;
  n_degree_cv = X_cv.^t;
  FX_train = [FX_train n_degree_train];
  FX_test = [FX_test n_degree_test];
  FX_cv = [FX_cv n_degree_cv];
  [m , n] = size(FX_train);
  init_theta = ones(n, 1);

[J, grad] = linearRegCostFunction(FX_train, y_train, init_theta, .1);
[theta] = trainLinearReg( FX_train, y_train, .1);
pred_train = FX_train * theta;
MSE_train = (sum((pred_train - y_train).^2)/train_size)
pred_test = FX_test* theta;
MSE_test = (sum((pred_test - y_test).^2)/test_size)
pred_cv = FX_cv* theta;
MSE_cv = (sum((pred_cv - y_cv).^2)/cv_size)

%%They all are pretty similar, so this means we have found a good fit that will
%%likely work just as well on new data
</pre></code>
									<p>The MSREs calculated for each set was pretty similar, so this is a good indication of how well this model will preform on new data.</p>
									<h2>Conclusion and Next Steps</h2>
									<p>The MSRE and NMSRE provide a way to compare different models to see which is a better fit, because they are a way of measuring the difference between the predicted and actual observed values. NMSRE values range from 0 to 1, where a 0 would mean that there was no difference between the predictions and observed values. The NMSRE value that I found is not particularly close to 0. In order to lower this value, a next possible step could be to try ignoring some of the features to see if some of them are actually hindering our fit. If this doesn't work, we could try to gather more data in order to have a larger training set. Whether or not we would take these steps depends on how accurate this model needs to be for the project's goals. This NMSRE may be satisfactory in some situations.</p>
							</section>

					</div>

				<!-- Footer -->
					<footer id="footer">
						<section>
							<h2>About Me</h2>
							<p>I am a data scientist from Montana with a background in physics and statistics. I am very interested in machine learning and big data analysis. Thanks for taking the time to look at my work. Please reach out if you have any questions for me. </p>
							<ul class="actions">
							</ul>
						</section>
						<section>
							<h2>Information</h2>
							<dl class="alt">
								<dt>Address</dt>
								<dd>Bozeman, MT &bull; USA</dd>
								<dt>Email</dt>
								<dd><a href="#">s.nagy.4343@gmail.com</a></dd>
							</dl>
							<ul class="icons">
								<li><a href="https://github.com/SarahMari" class="icon brands fa-github alt"><span class="label">GitHub</span></a></li>
								<li><a href= "http://linkedin.com/in/sarah-nagy-51412152" class = "icon brands fa-linkedin alt"><span class ="label"><Linkedin</span></a></li>
							</ul>
						</section>
						
					</footer>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
